The **training process** in the provided DQN training script for battery storage optimization follows these steps:

### 1. **Setup Environment and Model:**
   - The environment (`Battery`) is initialized with several parameters (e.g., battery capacity, efficiency, standby loss, etc.), along with the training data paths.
   - The model architecture is imported from external modules, with options for `DQN_Agent` (vanilla) or `DQN_Agent_double_duel` (for double or dueling DQN variants).
   - The training script supports different types of DQN models (`vanilla`, `double_dueling`, and `NN`).
   - The training loop is set to run for 12,000 episodes with 168 steps per episode (representing the time range).

### 2. **Epsilon-Greedy Exploration:**
   - The agent explores the environment using an epsilon-greedy strategy, where `epsilon` starts at 1.0 and decays over time (via `epsilon_decay`) until it reaches a minimum value (`epsilon_end`). For neural network-based models (e.g., `NN`, `double_dueling_NN`), epsilon is fixed at 0 to focus on exploiting the learned policy.

### 3. **Episode Loop:**
   - For each episode, the environment (`Battery`) is reset to its initial state.
   - Within each episode, the agent selects actions over a sequence of 168 time steps. The environment processes each action, returning the new state, reward, and whether the episode is finished (`done`).

### 4. **Agent-Environment Interaction:**
   - The agent interacts with the environment:
     - **Action Selection:** The agent chooses an action using the current state and the epsilon-greedy policy.
     - **Environment Step:** The environment processes the chosen action and updates its state according to battery dynamics (e.g., state of charge, price forecast, battery degradation).
     - **Agent Update:** The agent updates its knowledge by using the selected action's outcome (state, reward, next state, and whether the episode is done). This update uses the Q-learning step where the Q-values are updated based on the Bellman equation.

### 5. **Q-Network Update:**
   - The agent employs the DQN algorithm to learn Q-values for state-action pairs. It stores experiences in a replay buffer and updates its Q-network using mini-batch updates, which are performed every 16 steps.
   - The target network, which stabilizes training by slowly updating its weights from the main Q-network, is updated using a soft update rule with a parameter `tau = 0.001`.

### 6. **Profit Calculation:**
   - The profit generated by the battery system (based on charging and discharging actions) is tracked and stored for each episode in the variable `episode_profit`.
   - After each episode, the cumulative profit for the model is updated and stored for comparison across the different DQN models.

### 7. **Model Saving and Plotting:**
   - After all episodes are completed, the learned Q-network model for each DQN variant is saved to disk, along with the recorded rewards and profits.
   - A comparison of the cumulative profits for each DQN model is plotted to visualize the model's performance over time.

### 8. **Training Termination:**
   - The training process terminates after completing the set number of episodes (12,000), and results are saved for further analysis.

This process ensures that the agent learns to optimize battery storage operations by repeatedly interacting with the environment and adjusting its policy based on observed rewards (profits from charging/discharging the battery) and state transitions【11†source】【12†source】.
